{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flanaganc04/My-own-Neural-Net/blob/main/Digit_Recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "nceuyrNqsD2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVg2-ZWamHmU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import datetime\n",
        "# import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "1VLs8QoVsHuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and try not to get yelled at\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "PCOcf0EUkzoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet:\n",
        "  def __init__(self, neurons, eta):\n",
        "    self.neurons = neurons\n",
        "    self.eta = eta\n",
        "\n",
        "    self.weights_list = []              # [[W1], [W2], [W3], ...]\n",
        "    self.biases_list = []               # [[b1], [b2], [b3], ...]\n",
        "    self.layer_outputs_list = []        # [o1, o2, o3, ...]\n",
        "    self.activation_derivs_list = []    # [o1', o2', o3', ...]\n",
        "    self.weight_grads_list = []         # [dW1, dW2, dW3, ...]\n",
        "    self.bias_grads_list = []           # [db1, db2, db3, ...]\n",
        "\n",
        "    self.buildModel()\n",
        "\n",
        "  def readData(self, train_data, test_data):\n",
        "    X_train = train_data.iloc[:, 1:]\n",
        "    X_train_np = X_train.to_numpy() * (1 / 255)\n",
        "    Y_train = train_data.iloc[:, 0]\n",
        "    Y_train_np = Y_train.to_numpy()\n",
        "\n",
        "    X_test = test_data.iloc[:, 1:]\n",
        "    X_test_np = X_test.to_numpy() * (1 / 255)\n",
        "    Y_test = test_data.iloc[:, 0]\n",
        "    Y_test_np = Y_test.to_numpy()\n",
        "\n",
        "    return X_train_np, Y_train_np, X_test_np, Y_test_np\n",
        "\n",
        "  def init_weight_matrix(self, inputNeurons, outputNeurons):\n",
        "    weightMatrix = np.random.uniform(\n",
        "        low=-1.0, high=1.0, size=(inputNeurons, outputNeurons)\n",
        "    )\n",
        "    return weightMatrix\n",
        "\n",
        "  def init_bias_vector(self, neurons):\n",
        "    bias = np.zeros((1, neurons))\n",
        "    return bias\n",
        "\n",
        "  def matrixMult(self, matrix, weightMatrix):\n",
        "    product = matrix @ weightMatrix\n",
        "    return product\n",
        "\n",
        "  def softMax(self, matrix):\n",
        "    matrixTemp = matrix.copy()  # Create a copy to avoid modifying the input\n",
        "    expMatrix = np.exp(matrixTemp)\n",
        "    sums = expMatrix.sum(axis=1, keepdims=True)\n",
        "    softmax = expMatrix / (sums + 1e-9)  # avoids 0 division\n",
        "    derivative = softmax * (1 - softmax)\n",
        "    return softmax, derivative\n",
        "\n",
        "  def relu(self, matrix):\n",
        "    matrixTemp = matrix.copy()\n",
        "    relu = np.maximum(0, matrixTemp)\n",
        "    derivative = np.where(matrixTemp > 0, 1, 0)\n",
        "    return relu, derivative\n",
        "\n",
        "  def OneHotEncoder(self, matrix):\n",
        "    matrix = matrix.astype(int).flatten()\n",
        "    num_classes = 10\n",
        "    one_hot_matrix = np.eye(num_classes)[matrix]\n",
        "    return one_hot_matrix\n",
        "\n",
        "  def CCE(self, y_train):\n",
        "    # Gradient of the loss function\n",
        "    Y_train_onehot = self.OneHotEncoder(y_train)\n",
        "    output_square = Y_train_onehot\n",
        "\n",
        "    if self.layer_outputs_list[-1] is None:\n",
        "      raise RuntimeError(\"Run ForwardProp(X) before calling CCE.\")\n",
        "\n",
        "    # y_pred is last layer output\n",
        "    y_pred = self.layer_outputs_list[-1]\n",
        "    logOput = np.log(y_pred + 1e-9)  # avoid log(0)\n",
        "\n",
        "    CCE = -np.sum((output_square * logOput), axis=1, keepdims=True)\n",
        "    CCE_avg = np.mean(CCE)\n",
        "    return CCE_avg\n",
        "\n",
        "  def ForwardProp(self, X):\n",
        "    \"\"\"\n",
        "    X: (batch_size, input_dim)\n",
        "    \"\"\"\n",
        "    input = X\n",
        "    L = len(self.weights_list)\n",
        "\n",
        "    for i, (w, b) in enumerate(zip(self.weights_list, self.biases_list)):\n",
        "      # affine\n",
        "      z = self.matrixMult(input, w) + b\n",
        "\n",
        "      if i != L - 1:\n",
        "        # hidden layer: ReLU\n",
        "        a, da = self.relu(z)\n",
        "        self.layer_outputs_list[i] = a\n",
        "        self.activation_derivs_list[i] = da\n",
        "        input = a\n",
        "      else:\n",
        "        # output layer: Softmax\n",
        "        a, da = self.softMax(z)\n",
        "        self.layer_outputs_list[i] = a        # y_pred\n",
        "        self.activation_derivs_list[i] = da\n",
        "        input = a\n",
        "\n",
        "    return input  # y_pred\n",
        "\n",
        "  def BackProp(self, X_batch, y_batch):\n",
        "    \"\"\"\n",
        "    Whole-network backprop using the same logic as the layer-level `backward`.\n",
        "\n",
        "    X_batch: (m, n_input)\n",
        "    y_batch: (m,) integer labels 0–9\n",
        "    \"\"\"\n",
        "\n",
        "    # ----- 1. Forward pass to get predictions and cached activations -----\n",
        "    y_pred = self.ForwardProp(X_batch)       # shape: (m, 10)\n",
        "    m = X_batch.shape[0]\n",
        "\n",
        "    # ----- 2. Top gradient: dL/da at the output layer -----\n",
        "    # For Softmax + CCE, we have dL/dz = (y_pred - Y_true)/m.\n",
        "    # If we treat the activation at the last layer as \"identity\" for backprop,\n",
        "    # we can just start with dL_da = (y_pred - Y_true)/m and skip da_dz.\n",
        "    Y_true = self.OneHotEncoder(y_batch)     # shape: (m, 10)\n",
        "    dL_da = (y_pred - Y_true) / m            # this is our \"incoming\" gradient at last layer\n",
        "\n",
        "    L = len(self.weights_list)\n",
        "\n",
        "    # ----- 3. Iterate from last layer down to first -----\n",
        "    for layer in range(L - 1, -1, -1):\n",
        "        # ----- Identify layer-specific tensors -----\n",
        "        W = self.weights_list[layer]         # (input_size, output_size)\n",
        "\n",
        "        if layer == 0:\n",
        "            last_input = X_batch             # input to first layer\n",
        "        else:\n",
        "            last_input = self.layer_outputs_list[layer - 1]  # a^{(layer)}\n",
        "\n",
        "        # For hidden layers we stored f'(z) in activation_derivs_list.\n",
        "        # For the last (softmax+CCE) layer, we effectively treat f'(z) = 1\n",
        "        # because we already combined softmax + CCE into dL_da.\n",
        "        if layer == L - 1:\n",
        "            da_dz = 1.0                      # scalar broadcast; identity derivative\n",
        "        else:\n",
        "            da_dz = self.activation_derivs_list[layer]   # same shape as layer output\n",
        "\n",
        "        # ----- Apply the same logic as your `backward` -----\n",
        "        # dL_dz = dL/da * da/dz\n",
        "        dL_dz = dL_da * da_dz                # (m, output_size)\n",
        "\n",
        "        # gradient w.r.t. weights: last_input^T @ dL_dz\n",
        "        weight_gradient = last_input.T @ dL_dz / 1.0     # we already divided by m in dL_da\n",
        "\n",
        "        # gradient w.r.t. biases: sum over batch\n",
        "        bias_gradient = np.sum(dL_dz, axis=0, keepdims=True)\n",
        "\n",
        "        # gradient w.r.t. inputs: dL_dz @ W^T\n",
        "        dL_da = dL_dz @ W.T                  # this becomes dL/da for the previous layer\n",
        "\n",
        "        # store gradients\n",
        "        self.weight_grads_list[layer] = weight_gradient\n",
        "        self.bias_grads_list[layer]   = bias_gradient\n",
        "\n",
        "    # ----- 4. Gradient descent step (since you’re not using an external optimizer yet) -----\n",
        "    for i in range(L):\n",
        "        self.weights_list[i] -= self.eta * self.weight_grads_list[i]\n",
        "        self.biases_list[i]  -= self.eta * self.bias_grads_list[i]\n",
        "\n",
        "\n",
        "  def buildModel(self):\n",
        "    input_weights = self.init_weight_matrix(self.neurons[0], self.neurons[1])\n",
        "    self.weights_list.append(input_weights)\n",
        "    input_biases = self.init_bias_vector(self.neurons[1])\n",
        "    self.biases_list.append(input_biases)\n",
        "\n",
        "    hidden_weights = self.init_weight_matrix(self.neurons[1], self.neurons[2])\n",
        "    self.weights_list.append(hidden_weights)\n",
        "    hidden_biases = self.init_bias_vector(self.neurons[2])\n",
        "    self.biases_list.append(hidden_biases)\n",
        "\n",
        "    output_weights = self.init_weight_matrix(self.neurons[2], self.neurons[3])\n",
        "    self.weights_list.append(output_weights)\n",
        "    output_biases = self.init_bias_vector(self.neurons[3])\n",
        "    self.biases_list.append(output_biases)\n",
        "\n",
        "    # initialize storage for forward pass\n",
        "    L = len(self.weights_list)\n",
        "    self.layer_outputs_list = [None] * L\n",
        "    self.activation_derivs_list = [None] * L\n",
        "    self.weight_grads_list = [None] * L\n",
        "    self.bias_grads_list = [None] * L\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Runs a forward pass and returns predicted class labels.\n",
        "    \"\"\"\n",
        "    y_pred = self.ForwardProp(X)           # softmax outputs\n",
        "    return np.argmax(y_pred, axis=1)       # convert to class index\n",
        "\n",
        "\n",
        "def train(model, X_train, y_train, batch_size, epochs):\n",
        "    n_samples = X_train.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle indices\n",
        "        perm = np.random.permutation(n_samples)\n",
        "        X_shuf = X_train[perm]\n",
        "        y_shuf = y_train[perm]\n",
        "\n",
        "        epoch_loss_sum = 0.0\n",
        "\n",
        "        # tqdm progress bar over samples in this epoch\n",
        "        with tqdm(total=n_samples,\n",
        "                  desc=f\"Epoch {epoch+1}/{epochs}\",\n",
        "                  unit=\"sample\") as pbar:\n",
        "\n",
        "            for start in range(0, n_samples, batch_size):\n",
        "                end = min(start + batch_size, n_samples)\n",
        "                X_batch = X_shuf[start:end]\n",
        "                y_batch = y_shuf[start:end]\n",
        "\n",
        "                # One training step (forward + backward + update)\n",
        "                model.BackProp(X_batch, y_batch)\n",
        "\n",
        "                # Compute batch loss for display\n",
        "                _ = model.ForwardProp(X_batch)\n",
        "                batch_loss = model.CCE(y_batch)\n",
        "\n",
        "                batch_size_eff = end - start\n",
        "                epoch_loss_sum += batch_loss * batch_size_eff\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix(batch_loss=f\"{batch_loss:.4f}\")\n",
        "                pbar.update(batch_size_eff)\n",
        "\n",
        "        avg_epoch_loss = epoch_loss_sum / n_samples\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - avg loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "  \"\"\"\n",
        "  Computes accuracy on the test set.\n",
        "  \"\"\"\n",
        "  y_pred = model.predict(X_test)\n",
        "  accuracy = np.mean(y_pred == y_test)\n",
        "  print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "  return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "dhs3lYuQ8EDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Digit Recognizer/mnist_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Digit Recognizer/mnist_test.csv')\n",
        "\n",
        "neurons = [784,128,64,10]   #[n_1,n_2,n_2...n_i]\n",
        "eta = 0.1                   #learning rate"
      ],
      "metadata": {
        "id": "xrz5ku-MYAtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 5\n",
        ")\n",
        "\n",
        "# now test accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "rfwIC_3i-TIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = net.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(15,4.5))\n",
        "for i in range(20):\n",
        "    plt.subplot(3, 10, i+1)\n",
        "    plt.imshow(X_test[i].reshape((28,28)),cmap=plt.cm.binary)\n",
        "    plt.axis('off')\n",
        "plt.subplots_adjust(wspace=-0.1, hspace=-0.1)\n",
        "\n",
        "print(pred[0:10])\n",
        "print(pred[10:20])\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "k0jVAtT88_Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "k5zy8tIJ8TO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 10\n",
        ")\n",
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "awa4umg2Veqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 20\n",
        ")\n",
        "evaluate(net, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "OcM_6L91VjzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 50\n",
        ")\n",
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "h0o76HOYYgCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 64,\n",
        "    epochs = 100\n",
        ")\n",
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "bXaA4Of9Ykqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 5\n",
        ")\n",
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "CWYLq5hdZWQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 32,\n",
        "    epochs = 5\n",
        ")\n",
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "D51mpP5tZZEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 128,\n",
        "    epochs = 5\n",
        ")\n",
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "Rc1kCTTUZbTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(neurons = neurons, eta = eta)\n",
        "X_train, y_train, X_test, y_test = net.readData(train_data, test_data)\n",
        "\n",
        "train(\n",
        "    model = net,\n",
        "    X_train = X_train,\n",
        "    y_train = y_train,\n",
        "    batch_size = 256,\n",
        "    epochs = 5\n",
        ")\n",
        "evaluate(net, X_test, y_test)"
      ],
      "metadata": {
        "id": "agvw5GI9Zdmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [5, 10, 20, 50, 100]\n",
        "batches = [16, 32 ,64, 128, 256]\n",
        "epoch_accuracies = [0.8983, 0.9342, 0.9472, 0.9556, 0.9568]\n",
        "batch_accuracies = [0.945, 0.9337, 0.8983, 0.8809, 0.8362]"
      ],
      "metadata": {
        "id": "H9BKu-hOYl8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, epoch_accuracies, 'bo', linestyle = '--')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')"
      ],
      "metadata": {
        "id": "xIV2EWqEZPS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(batches, batch_accuracies, 'bo', linestyle = '--')\n",
        "plt.xlabel('Batches')\n",
        "plt.ylabel('Accuracy')"
      ],
      "metadata": {
        "id": "PNiaFtWfjSv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}